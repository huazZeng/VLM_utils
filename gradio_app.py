#!/usr/bin/env python3
"""
Gradioå‰ç«¯ç•Œé¢ for VLMæ¨ç†å·¥å…·
æ”¯æŒå•ä¸ªæ¨ç†å’Œæ‰¹é‡æ¨ç†
"""

import gradio as gr
import os
import json
import tempfile
from typing import List, Dict, Any
from inference import UnifiedInference


class GradioInferenceApp:
    """
    Gradioæ¨ç†åº”ç”¨ç±»
    """
    
    def __init__(self):
        self.inference = None
        self.current_engine_type = None
        self.current_model_name = None
        
    def initialize_engine(self, engine_type: str, model_name: str, **kwargs):
        """
        åˆå§‹åŒ–æ¨ç†å¼•æ“
        
        Args:
            engine_type: å¼•æ“ç±»å‹
            model_name: æ¨¡å‹åç§°
            **kwargs: å…¶ä»–å‚æ•°
        """
        try:
            # å¦‚æœå¼•æ“ç±»å‹æˆ–æ¨¡å‹åç§°æ”¹å˜ï¼Œé‡æ–°åˆå§‹åŒ–
            if (self.current_engine_type != engine_type or 
                self.current_model_name != model_name):
                
                engine_kwargs = {
                    "model_name": model_name,
                    **kwargs
                }
                
                self.inference = UnifiedInference(
                    parser=None,
                    engine_type=engine_type,
                    **engine_kwargs
                )
                
                self.current_engine_type = engine_type
                self.current_model_name = model_name
                
                return f"âœ… å¼•æ“åˆå§‹åŒ–æˆåŠŸ: {engine_type} - {model_name}"
            else:
                return f"â„¹ï¸ å¼•æ“å·²å­˜åœ¨: {engine_type} - {model_name}"
                
        except Exception as e:
            return f"âŒ å¼•æ“åˆå§‹åŒ–å¤±è´¥: {str(e)}"
    
    def single_inference(self, image, prompt: str, engine_type: str, model_name: str, 
                        system_prompt: str = "You are a helpful assistant.",
                        max_tokens: int = 1024, temperature: float = 0.0):
        """
        å•ä¸ªæ¨ç†
        
        Args:
            image: ä¸Šä¼ çš„å›¾åƒ
            prompt: æç¤ºæ–‡æœ¬
            engine_type: å¼•æ“ç±»å‹
            model_name: æ¨¡å‹åç§°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            æ¨ç†ç»“æœ
        """
        if image is None:
            return "âŒ è¯·ä¸Šä¼ å›¾åƒ"
        
        if not prompt.strip():
            return "âŒ è¯·è¾“å…¥æç¤ºæ–‡æœ¬"
        
        try:
            # åˆå§‹åŒ–å¼•æ“
            init_result = self.initialize_engine(
                engine_type=engine_type,
                model_name=model_name,
                system_prompt=system_prompt,
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            if "âŒ" in init_result:
                return init_result
            
            # ä¿å­˜ä¸´æ—¶å›¾åƒæ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix=".jpg", delete=False) as tmp_file:
                image.save(tmp_file.name)
                temp_image_path = tmp_file.name
            
            try:
                # æ‰§è¡Œæ¨ç†
                result = self.inference.single_infer(temp_image_path, prompt)
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.unlink(temp_image_path)
                
                return f"âœ… æ¨ç†å®Œæˆ\n\n**ç»“æœ:**\n{result}"
                
            except Exception as e:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_image_path):
                    os.unlink(temp_image_path)
                raise e
                
        except Exception as e:
            return f"âŒ æ¨ç†å¤±è´¥: {str(e)}"
    
    def batch_inference(self, files, engine_type: str, model_name: str,
                       system_prompt: str = "You are a helpful assistant.",
                       user_prompt: str = "Describe the image in detail.",
                       max_tokens: int = 1024, temperature: float = 0.0,
                       batch_size: int = 16):
        """
        æ‰¹é‡æ¨ç†
        
        Args:
            files: ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
            engine_type: å¼•æ“ç±»å‹
            model_name: æ¨¡å‹åç§°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            æ‰¹é‡æ¨ç†ç»“æœ
        """
        if not files:
            return "âŒ è¯·ä¸Šä¼ æ–‡ä»¶"
        
        try:
            # åˆå§‹åŒ–å¼•æ“
            init_result = self.initialize_engine(
                engine_type=engine_type,
                model_name=model_name,
                system_prompt=system_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                batch_size=batch_size
            )
            
            if "âŒ" in init_result:
                return init_result
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            with tempfile.TemporaryDirectory() as temp_dir:
                # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                image_paths = []
                for file in files:
                    if file.name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):
                        # å›¾åƒæ–‡ä»¶
                        temp_path = os.path.join(temp_dir, os.path.basename(file.name))
                        with open(temp_path, 'wb') as f:
                            f.write(file.read())
                        image_paths.append(temp_path)
                    elif file.name.lower().endswith('.json'):
                        # JSONæ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨
                        temp_path = os.path.join(temp_dir, os.path.basename(file.name))
                        with open(temp_path, 'wb') as f:
                            f.write(file.read())
                        image_paths.append(temp_path)
                
                if not image_paths:
                    return "âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å›¾åƒæˆ–JSONæ–‡ä»¶"
                
                # æ‰§è¡Œæ‰¹é‡æ¨ç†
                output_dir = os.path.join(temp_dir, "results")
                os.makedirs(output_dir, exist_ok=True)
                
                if len(image_paths) == 1 and image_paths[0].endswith('.json'):
                    # JSONæ–‡ä»¶æ‰¹é‡æ¨ç†
                    self.inference.batch_infer(image_paths[0], output_dir, "all")
                else:
                    # å›¾åƒæ–‡ä»¶æ‰¹é‡æ¨ç†
                    self.inference.batch_infer(temp_dir, output_dir, "all")
                
                # è¯»å–ç»“æœ
                results = []
                for file in os.listdir(output_dir):
                    if file.endswith('.json'):
                        with open(os.path.join(output_dir, file), 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            if isinstance(data, list):
                                results.extend(data)
                            else:
                                results.append(data)
                
                # æ ¼å¼åŒ–è¾“å‡º
                output = "âœ… æ‰¹é‡æ¨ç†å®Œæˆ\n\n"
                output += f"**å¤„ç†æ–‡ä»¶æ•°:** {len(image_paths)}\n"
                output += f"**ç»“æœæ•°:** {len(results)}\n\n"
                
                for i, result in enumerate(results[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ªç»“æœ
                    output += f"**ç»“æœ {i+1}:**\n"
                    output += f"å›¾åƒ: {result.get('image_path', 'N/A')}\n"
                    output += f"æç¤º: {result.get('prompt', 'N/A')}\n"
                    output += f"é¢„æµ‹: {result.get('prediction', 'N/A')}\n"
                    if 'ground_truth' in result:
                        output += f"çœŸå€¼: {result.get('ground_truth', 'N/A')}\n"
                    output += "\n"
                
                if len(results) > 10:
                    output += f"... è¿˜æœ‰ {len(results) - 10} ä¸ªç»“æœ\n"
                
                return output
                
        except Exception as e:
            return f"âŒ æ‰¹é‡æ¨ç†å¤±è´¥: {str(e)}"


def create_gradio_interface():
    """
    åˆ›å»ºGradioç•Œé¢
    """
    app = GradioInferenceApp()
    
    # å¼•æ“é…ç½®
    engine_types = ["transformer", "vllm_offline", "api_chat", "api_completion"]
    
    with gr.Blocks(title="VLMæ¨ç†å·¥å…·", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# ğŸ¤– VLMæ¨ç†å·¥å…·")
        gr.Markdown("æ”¯æŒå¤šç§æ¨ç†å¼•æ“çš„ç»Ÿä¸€æ¨ç†ç•Œé¢")
        
        with gr.Tab("å•ä¸ªæ¨ç†"):
            with gr.Row():
                with gr.Column(scale=1):
                    # å¼•æ“é…ç½®
                    gr.Markdown("### ğŸ”§ å¼•æ“é…ç½®")
                    engine_type = gr.Dropdown(
                        choices=engine_types,
                        value="transformer",
                        label="æ¨ç†å¼•æ“ç±»å‹"
                    )
                    model_name = gr.Textbox(
                        value="/mnt/petrelfs/zenghuazheng/workspace/models/models/qwenvl2.5-3b",
                        label="æ¨¡å‹åç§°/è·¯å¾„"
                    )
                    system_prompt = gr.Textbox(
                        value="You are a helpful assistant.",
                        label="ç³»ç»Ÿæç¤ºè¯",
                        lines=2
                    )
                    
                    with gr.Row():
                        max_tokens = gr.Slider(
                            minimum=1, maximum=4096, value=1024, step=1,
                            label="æœ€å¤§Tokenæ•°"
                        )
                        temperature = gr.Slider(
                            minimum=0.0, maximum=2.0, value=0.0, step=0.1,
                            label="æ¸©åº¦å‚æ•°"
                        )
                    
                    # åˆå§‹åŒ–æŒ‰é’®
                    init_btn = gr.Button("ğŸš€ åˆå§‹åŒ–å¼•æ“", variant="primary")
                    init_output = gr.Textbox(label="åˆå§‹åŒ–çŠ¶æ€", interactive=False)
                
                with gr.Column(scale=1):
                    # æ¨ç†è¾“å…¥
                    gr.Markdown("### ğŸ“¸ æ¨ç†è¾“å…¥")
                    image_input = gr.Image(
                        label="ä¸Šä¼ å›¾åƒ",
                        type="pil"
                    )
                    prompt_input = gr.Textbox(
                        value="<image>Find the spectral images in the figure",
                        label="æç¤ºæ–‡æœ¬",
                        lines=3
                    )
                    
                    # æ¨ç†æŒ‰é’®
                    infer_btn = gr.Button("ğŸ” å¼€å§‹æ¨ç†", variant="primary", size="lg")
            
            # æ¨ç†ç»“æœ
            gr.Markdown("### ğŸ“Š æ¨ç†ç»“æœ")
            result_output = gr.Markdown(label="æ¨ç†ç»“æœ")
        
        with gr.Tab("æ‰¹é‡æ¨ç†"):
            with gr.Row():
                with gr.Column(scale=1):
                    # å¼•æ“é…ç½®
                    gr.Markdown("### ğŸ”§ å¼•æ“é…ç½®")
                    batch_engine_type = gr.Dropdown(
                        choices=engine_types,
                        value="transformer",
                        label="æ¨ç†å¼•æ“ç±»å‹"
                    )
                    batch_model_name = gr.Textbox(
                        value="/mnt/petrelfs/zenghuazheng/workspace/models/models/qwenvl2.5-3b",
                        label="æ¨¡å‹åç§°/è·¯å¾„"
                    )
                    batch_system_prompt = gr.Textbox(
                        value="You are a helpful assistant.",
                        label="ç³»ç»Ÿæç¤ºè¯",
                        lines=2
                    )
                    batch_user_prompt = gr.Textbox(
                        value="Describe the image in detail.",
                        label="ç”¨æˆ·æç¤ºè¯",
                        lines=2
                    )
                    
                    with gr.Row():
                        batch_max_tokens = gr.Slider(
                            minimum=1, maximum=4096, value=1024, step=1,
                            label="æœ€å¤§Tokenæ•°"
                        )
                        batch_temperature = gr.Slider(
                            minimum=0.0, maximum=2.0, value=0.0, step=0.1,
                            label="æ¸©åº¦å‚æ•°"
                        )
                        batch_size = gr.Slider(
                            minimum=1, maximum=64, value=16, step=1,
                            label="æ‰¹å¤„ç†å¤§å°"
                        )
                
                with gr.Column(scale=1):
                    # æ‰¹é‡è¾“å…¥
                    gr.Markdown("### ğŸ“ æ‰¹é‡è¾“å…¥")
                    gr.Markdown("æ”¯æŒä¸Šä¼ å¤šä¸ªå›¾åƒæ–‡ä»¶æˆ–ä¸€ä¸ªJSONæ–‡ä»¶")
                    files_input = gr.File(
                        label="ä¸Šä¼ æ–‡ä»¶",
                        file_count="multiple",
                        file_types=["image", "json"]
                    )
                    
                    # æ‰¹é‡æ¨ç†æŒ‰é’®
                    batch_infer_btn = gr.Button("ğŸš€ å¼€å§‹æ‰¹é‡æ¨ç†", variant="primary", size="lg")
            
            # æ‰¹é‡æ¨ç†ç»“æœ
            gr.Markdown("### ğŸ“Š æ‰¹é‡æ¨ç†ç»“æœ")
            batch_result_output = gr.Markdown(label="æ‰¹é‡æ¨ç†ç»“æœ")
        
        # äº‹ä»¶ç»‘å®š
        init_btn.click(
            fn=app.initialize_engine,
            inputs=[engine_type, model_name, system_prompt, max_tokens, temperature],
            outputs=init_output
        )
        
        infer_btn.click(
            fn=app.single_inference,
            inputs=[image_input, prompt_input, engine_type, model_name, 
                   system_prompt, max_tokens, temperature],
            outputs=result_output
        )
        
        batch_infer_btn.click(
            fn=app.batch_inference,
            inputs=[files_input, batch_engine_type, batch_model_name,
                   batch_system_prompt, batch_user_prompt,
                   batch_max_tokens, batch_temperature, batch_size],
            outputs=batch_result_output
        )
        
        # ç¤ºä¾‹
        gr.Markdown("### ğŸ’¡ ä½¿ç”¨è¯´æ˜")
        gr.Markdown("""
        1. **å•ä¸ªæ¨ç†**: ä¸Šä¼ ä¸€å¼ å›¾åƒï¼Œè¾“å…¥æç¤ºæ–‡æœ¬ï¼Œç‚¹å‡»æ¨ç†
        2. **æ‰¹é‡æ¨ç†**: ä¸Šä¼ å¤šä¸ªå›¾åƒæ–‡ä»¶æˆ–ä¸€ä¸ªJSONæ–‡ä»¶è¿›è¡Œæ‰¹é‡å¤„ç†
        3. **å¼•æ“é…ç½®**: é€‰æ‹©åˆé€‚çš„æ¨ç†å¼•æ“å’Œæ¨¡å‹
        4. **å‚æ•°è°ƒæ•´**: æ ¹æ®éœ€è¦è°ƒæ•´æ¸©åº¦ã€æœ€å¤§tokenæ•°ç­‰å‚æ•°
        """)
    
    return interface


if __name__ == "__main__":
    # åˆ›å»ºå¹¶å¯åŠ¨Gradioç•Œé¢
    interface = create_gradio_interface()
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,
        debug=True
    ) 